# ══════════════════════════════════════════════════════════════════════════════
# KUBERNETES DEPLOYMENT — AI Calling Agent
# ══════════════════════════════════════════════════════════════════════════════
# kubectl apply -f deploy/k8s/
# ══════════════════════════════════════════════════════════════════════════════

---
# ── NAMESPACE ────────────────────────────────────────────────────────────────
apiVersion: v1
kind: Namespace
metadata:
  name: ai-calling

---
# ── SECRETS (create before deployment) ───────────────────────────────────────
# kubectl create secret generic app-secrets -n ai-calling \
#   --from-literal=PLIVO_AUTH_ID=MA... \
#   --from-literal=PLIVO_AUTH_TOKEN=... \
#   --from-literal=OPENAI_API_KEY=sk-... \
#   --from-literal=S3_ACCESS_KEY=... \
#   --from-literal=S3_SECRET_KEY=... \
#   --from-literal=MONGODB_URI=mongodb+srv://...
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
  namespace: ai-calling
type: Opaque
data:
  # Base64 encoded values — REPLACE with your actual secrets
  # echo -n "your-value" | base64
  PLIVO_AUTH_ID: REPLACE_BASE64
  PLIVO_AUTH_TOKEN: REPLACE_BASE64
  PLIVO_CALLER_ID: REPLACE_BASE64
  OPENAI_API_KEY: REPLACE_BASE64
  MONGODB_URI: REPLACE_BASE64
  S3_BUCKET: REPLACE_BASE64
  S3_REGION: REPLACE_BASE64
  S3_ACCESS_KEY: REPLACE_BASE64
  S3_SECRET_KEY: REPLACE_BASE64
  S3_ENDPOINT: REPLACE_BASE64

---
# ── CONFIGMAP (non-sensitive config) ─────────────────────────────────────────
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: ai-calling
data:
  NODE_ENV: "production"
  PORT: "3000"
  HOST: "0.0.0.0"
  CALL_MAX_MINUTES: "10"
  COMPANY_NAME: "Premier Realty Group"
  AGENT_NAME: "Priya"
  CORS_ORIGINS: "https://your-frontend.com"
  LOG_LEVEL: "info"

---
# ── DEPLOYMENT ───────────────────────────────────────────────────────────────
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-calling-agent
  namespace: ai-calling
  labels:
    app: ai-calling-agent
    version: v2.0.0
spec:
  replicas: 2
  revisionHistoryLimit: 5 # Keep 5 revisions for rollback
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1 # Add 1 pod at a time
      maxUnavailable: 0 # Never have fewer than desired replicas
  selector:
    matchLabels:
      app: ai-calling-agent
  template:
    metadata:
      labels:
        app: ai-calling-agent
        version: v2.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"
        prometheus.io/path: "/api/v1/metrics"
    spec:
      # ── Anti-affinity: spread pods across nodes ────────────────────────
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - ai-calling-agent
                topologyKey: kubernetes.io/hostname

      # ── Graceful shutdown: wait for in-flight calls ────────────────────
      terminationGracePeriodSeconds: 60

      containers:
        - name: ai-calling-agent
          image: your-registry.com/ai-calling-agent:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 3000
              name: http
              protocol: TCP

          # ── Environment from ConfigMap + Secrets ─────────────────────
          envFrom:
            - configMapRef:
                name: app-config
            - secretRef:
                name: app-secrets

          # ── Resource limits ──────────────────────────────────────────
          resources:
            requests:
              cpu: 250m # 0.25 CPU — baseline for idle
              memory: 256Mi
            limits:
              cpu: "1" # 1 full CPU — burst for STT/TTS processing
              memory: 512Mi # Hard cap — OOMKill if exceeded

          # ── Liveness probe (is the process alive?) ───────────────────
          livenessProbe:
            httpGet:
              path: /health
              port: 3000
            initialDelaySeconds: 10
            periodSeconds: 15
            timeoutSeconds: 3
            failureThreshold: 3

          # ── Readiness probe (can it accept traffic?) ─────────────────
          # K8s removes pod from Service endpoints if this fails
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 3000
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 2

          # ── Startup probe (give time for initial setup) ──────────────
          startupProbe:
            httpGet:
              path: /health
              port: 3000
            initialDelaySeconds: 3
            periodSeconds: 5
            failureThreshold: 10 # Up to 50 seconds to start

          # ── Lifecycle hooks ──────────────────────────────────────────
          lifecycle:
            preStop:
              exec:
                # Give LB time to stop routing traffic before shutdown
                command: ["sh", "-c", "sleep 5"]

---
# ── SERVICE (ClusterIP — behind Ingress) ─────────────────────────────────────
apiVersion: v1
kind: Service
metadata:
  name: ai-calling-svc
  namespace: ai-calling
  labels:
    app: ai-calling-agent
spec:
  type: ClusterIP
  selector:
    app: ai-calling-agent
  ports:
    - name: http
      port: 80
      targetPort: 3000
      protocol: TCP

---
# ── INGRESS (with WebSocket support) ─────────────────────────────────────────
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ai-calling-ingress
  namespace: ai-calling
  annotations:
    # ── For nginx-ingress controller ─────────────────────────────────
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "900" # 15 min for WS
    nginx.ingress.kubernetes.io/proxy-send-timeout: "900"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "10"

    # WebSocket support
    nginx.ingress.kubernetes.io/websocket-services: "ai-calling-svc"
    nginx.ingress.kubernetes.io/upstream-hash-by: "$arg_callSid"

    # SSL
    cert-manager.io/cluster-issuer: "letsencrypt-prod"

    # Rate limiting
    nginx.ingress.kubernetes.io/limit-rps: "20"
    nginx.ingress.kubernetes.io/limit-burst-multiplier: "3"

    # ── For AWS ALB controller (alternative) ─────────────────────────
    # alb.ingress.kubernetes.io/scheme: internet-facing
    # alb.ingress.kubernetes.io/target-type: ip
    # alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    # alb.ingress.kubernetes.io/idle-timeout: "900"
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - api.your-domain.com
      secretName: ai-calling-tls
  rules:
    - host: api.your-domain.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: ai-calling-svc
                port:
                  number: 80

---
# ── HORIZONTAL POD AUTOSCALER ────────────────────────────────────────────────
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-calling-hpa
  namespace: ai-calling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-calling-agent
  minReplicas: 2
  maxReplicas: 20
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60 # Add up to 2 pods per minute
    scaleDown:
      stabilizationWindowSeconds: 300 # Wait 5 min before scaling down
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120 # Remove 1 pod every 2 min
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60 # Scale at 60% CPU (voice is latency-sensitive)
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70

---
# ── POD DISRUPTION BUDGET ────────────────────────────────────────────────────
# Ensures at least 1 pod is always available during voluntary disruptions
# (node drain, cluster upgrade, etc.)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ai-calling-pdb
  namespace: ai-calling
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: ai-calling-agent
