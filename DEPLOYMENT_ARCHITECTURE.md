# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI Calling Agent â€” Production Deployment Architecture
# Senior DevOps Architect Audit Report
# Date: 2026-02-14 | Version 2.0.0
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

---

## EXECUTIVE SUMMARY

This report covers the complete production deployment architecture for a
**real-time AI voice system** with Twilio integration. Real-time voice has
fundamentally different infrastructure requirements than typical web apps:

- **WebSocket connections last 1-10 minutes** (not milliseconds)
- **Audio streams at 20 packets/sec** (not request-response)
- **Latency budget is 3 seconds** (not best-effort)
- **A dropped connection = a dropped phone call** (user-facing failure)

The architecture below achieves:
- âœ… **Zero downtime deploys** (rolling update + connection draining)
- âœ… **Auto-scaling** (2-20 pods based on CPU, with voice-aware tuning)
- âœ… **Secure environment** (secrets management, TLS, non-root containers)
- âœ… **Low latency** (region co-location with Twilio, keep-alive pools)
- âœ… **High availability** (multi-AZ, PDB, anti-affinity, health probes)

---

## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## 1. INFRASTRUCTURE ARCHITECTURE
## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 1.1 Production Infrastructure Diagram

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚           INTERNET / PSTN                â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                    â”‚                         â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚  Twilio    â”‚      â”‚  Twilio     â”‚          â”‚  Frontend   â”‚
              â”‚  Voice     â”‚      â”‚  Media      â”‚          â”‚  Dashboard  â”‚
              â”‚  Webhooks  â”‚      â”‚  Streams    â”‚          â”‚  (Next.js)  â”‚
              â”‚  (HTTPS)   â”‚      â”‚  (WSS)      â”‚          â”‚  (Vercel)   â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                    â”‚                         â”‚
                    â–¼                    â–¼                         â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    CLOUDFLARE / AWS ALB                       â”‚
        â”‚              (SSL Termination, DDoS Protection)              â”‚
        â”‚                                                               â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚   â”‚  Nginx Ingress Controller / ALB Target Group         â”‚    â”‚
        â”‚   â”‚  - WebSocket upgrade for /stream (15 min timeout)   â”‚    â”‚
        â”‚   â”‚  - Rate limiting: API 10r/s, Webhooks 50r/s         â”‚    â”‚
        â”‚   â”‚  - Sticky sessions by callSid (WS routing)          â”‚    â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                KUBERNETES CLUSTER (EKS / GKE)                â”‚
        â”‚                                                               â”‚
        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â”‚   â”‚          ai-calling Namespace                         â”‚    â”‚
        â”‚   â”‚                                                       â”‚    â”‚
        â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
        â”‚   â”‚  â”‚  Pod #1   â”‚  â”‚  Pod #2   â”‚  â”‚  Pod #N   â”‚  â†HPA   â”‚    â”‚
        â”‚   â”‚  â”‚  Node.js  â”‚  â”‚  Node.js  â”‚  â”‚  Node.js  â”‚  2-20   â”‚    â”‚
        â”‚   â”‚  â”‚  256Mi    â”‚  â”‚  256Mi    â”‚  â”‚  256Mi    â”‚          â”‚    â”‚
        â”‚   â”‚  â”‚  0.25 CPU â”‚  â”‚  0.25 CPU â”‚  â”‚  0.25 CPU â”‚          â”‚    â”‚
        â”‚   â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚    â”‚
        â”‚   â”‚        â”‚              â”‚              â”‚                â”‚    â”‚
        â”‚   â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚    â”‚
        â”‚   â”‚                       â”‚                                â”‚    â”‚
        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
        â”‚                           â”‚                                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                â”‚                    â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  MongoDB    â”‚  â”‚  Redis      â”‚  â”‚  Cloudflare R2  â”‚
            â”‚  Atlas M10  â”‚  â”‚  (ElastiC.) â”‚  â”‚  (S3 compat.)   â”‚
            â”‚  Multi-AZ   â”‚  â”‚  Rate limit â”‚  â”‚  TTS audio      â”‚
            â”‚  Auto backupâ”‚  â”‚  Session    â”‚  â”‚  Recordings     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                               â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                               â”‚  Cloudflare  â”‚
                                               â”‚  CDN (audio) â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Region Placement Strategy

**Goal:** Minimize latency between your server and Twilio's media servers.

| Component | Recommended Region | Rationale |
|-----------|-------------------|-----------|
| App servers | **Mumbai (ap-south-1)** or **US-East (us-east-1)** | Closest to your callers and Twilio's India/US media servers |
| MongoDB Atlas | **Same region** as app | < 5ms roundtrip for DB queries |
| Redis | **Same region** | < 1ms for rate limiting |
| Cloudflare R2 | **Auto (global)** | Edge-cached TTS audio |
| OpenAI API | **N/A (fixed)** | OpenAI is US-based; ~150ms from India |

> **CRITICAL:** Twilio Media Streams connect from Twilio's data center to YOUR
> WebSocket endpoint. If your server is in Mumbai and Twilio's media server is
> in Virginia, you add ~200ms of round-trip latency to EVERY audio packet.
>
> **Recommendation for Indian callers:** Use `ap-south-1` (Mumbai) â€” Twilio
> has a regional media server in Singapore/India.

### 1.3 Load Balancing for WebSocket

**The #1 mistake:** Generic HTTP load balancers break WebSocket connections.

| Requirement | Solution |
|-------------|----------|
| WebSocket upgrade | ALB/Nginx with `proxy_set_header Upgrade` |
| Long-lived connections | Idle timeout â‰¥ 900s (15 min) |
| Sticky routing | Hash by `callSid` query param |
| Health-aware routing | Readiness probe removes unhealthy pods |
| Connection draining | `terminationGracePeriodSeconds: 60` |

**Why sticky routing matters:**
A single call's audio stream MUST hit the SAME server for the entire duration.
The `CallSession` state (audio buffer, conversation history, lead data) is
in-memory. If the LB routes packets to a different server, the call breaks.

```nginx
# Nginx upstream hash for sticky WebSocket routing
upstream app_backend {
    hash $arg_callSid consistent;
    server pod1:3000;
    server pod2:3000;
}
```

---

## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## 2. CONTAINER OPTIMIZATION
## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 2.1 Dockerfile Audit

| Issue Found | Severity | Fix Applied |
|------------|----------|-------------|
| Single-stage build (~240MB) | ğŸŸ  HIGH | Multi-stage: deps stage + production stage (~95MB) |
| Running as root | ğŸ”´ CRITICAL | `adduser -S appuser` + `USER appuser` |
| .env baked into image | ğŸ”´ CRITICAL | Removed â€” secrets from orchestrator env vars |
| No init system (PID 1 issue) | ğŸŸ  HIGH | Added `tini` as entrypoint |
| `npm install` (not `npm ci`) | ğŸŸ¡ MEDIUM | Changed to `npm ci --omit=dev` |
| No .dockerignore | ğŸŸ¡ MEDIUM | Created â€” excludes node_modules, .git, frontend |
| Health check uses Node.js | ğŸŸ¡ MEDIUM | Changed to `curl` (faster, lower memory) |
| No resource limits | ğŸŸ  HIGH | Added in docker-compose and K8s manifests |

### 2.2 Image Size Comparison

```
BEFORE (single stage, node:18):
  node:18           â†’  900MB base
  + npm install     â†’  240MB deps
  + source code     â†’   50MB
  TOTAL:            ~1190MB  âŒ

AFTER (multi-stage, node:20-alpine):
  node:20-alpine    â†’   120MB base
  + npm ci --omit=dev â†’ 65MB deps
  + source code     â†’    5MB
  + tini + curl     â†’    5MB
  TOTAL:            ~  95MB  âœ…  (12x smaller)
```

### 2.3 Startup Time

```
BEFORE: ~8-12 seconds
  - npm install at runtime: 5-8s
  - MongoDB connection: 2-3s
  - Config loading: <1s

AFTER: ~3-5 seconds
  - Deps pre-installed in image: 0s
  - MongoDB connection: 2-3s
  - Config validation: <100ms
  - Twilio client init: <100ms
```

### 2.4 PID 1 Problem (CRITICAL for Docker)

Node.js doesn't handle signals correctly when running as PID 1 in a container.
`SIGTERM` from Docker/K8s is silently ignored, causing:
- 10-second forced kill (no graceful shutdown)
- Active calls dropped without saving transcripts
- Database connections not cleaned up

**Fix:** `tini` as init system:
```dockerfile
RUN apk add --no-cache tini
ENTRYPOINT ["tini", "--"]
CMD ["node", "src/server.js"]
```

---

## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## 3. SECURITY HARDENING
## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 3.1 Security Audit Results

| Layer | Check | Status | Notes |
|-------|-------|--------|-------|
| **Transport** | HTTPS enforcement | âœ… | HSTS header + HTTPâ†’HTTPS redirect |
| **Transport** | TLS 1.2+ only | âœ… | Nginx config: `ssl_protocols TLSv1.2 TLSv1.3` |
| **Transport** | WebSocket over WSS | âœ… | SSL termination at LB/Nginx |
| **Auth** | Twilio webhook signature | âœ… | `twilio.validateRequest()` in production |
| **Auth** | API authentication | âš ï¸ | Rate limiting only â€” add API keys for production |
| **Secrets** | No .env in image | âœ… | Secrets via env vars from orchestrator |
| **Secrets** | No secrets in logs | âœ… | Config keys not logged |
| **Secrets** | K8s Secrets | âœ… | Base64 encoded in Secret resource |
| **Container** | Non-root user | âœ… | `appuser:appgroup` |
| **Container** | Read-only FS | âš ï¸ | Not yet â€” add `readOnlyRootFilesystem: true` |
| **Network** | Firewall rules | âœ… | ClusterIP service â€” not directly exposed |
| **Network** | DDoS protection | âœ… | Cloudflare + Nginx rate limiting |
| **Headers** | Security headers | âœ… | X-Content-Type-Options, X-Frame-Options, HSTS |

### 3.2 Secret Management Strategy

```
DEVELOPMENT:     .env file (gitignored)
STAGING/PROD:    External secrets manager

Recommended hierarchy:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AWS Secrets Manager / HashiCorp Vault           â”‚
â”‚                                                    â”‚
â”‚  ai-calling-agent/prod:                           â”‚
â”‚    TWILIO_ACCOUNT_SID: AC...                      â”‚
â”‚    TWILIO_AUTH_TOKEN: ...                          â”‚
â”‚    OPENAI_API_KEY: sk-...                         â”‚
â”‚    MONGODB_URI: mongodb+srv://...                 â”‚
â”‚    S3_ACCESS_KEY: ...                             â”‚
â”‚    S3_SECRET_KEY: ...                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kubernetes External Secrets Operator            â”‚
â”‚  or                                               â”‚
â”‚  AWS ECS Task Definition secretsManager          â”‚
â”‚  or                                               â”‚
â”‚  Railway/Render environment variables             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              Container env vars
```

### 3.3 Firewall Rules

```
INBOUND:
  Port 443 (HTTPS)     â† Internet (via LB)
  Port 443 (WSS)       â† Twilio Media Servers
  Port 3000            â† LB â†’ App pods (internal only)

OUTBOUND:
  api.openai.com:443   â†’ OpenAI STT/LLM/TTS
  api.twilio.com:443   â†’ Twilio REST API
  MongoDB Atlas:27017  â†’ Database
  R2/S3 endpoint:443   â†’ Object storage

BLOCKED:
  All other inbound    â† Security group / NetworkPolicy
  SSH (port 22)        â† Disabled in production containers
```

### 3.4 Rate Limiting Architecture

```
Layer 1: Cloudflare    â†’ 1000 req/s per IP (DDoS protection)
Layer 2: Nginx         â†’ API: 10 req/s, Webhooks: 50 req/s
Layer 3: Application   â†’ 200 req/min per IP (in-memory)
Layer 4: OpenAI        â†’ Circuit breaker (5 failures â†’ open for 60s)
```

---

## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## 4. CI/CD PIPELINE
## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 4.1 Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Push    â”‚â”€â”€â”€â–¶â”‚   TEST   â”‚â”€â”€â”€â–¶â”‚  BUILD   â”‚â”€â”€â”€â–¶â”‚   STAGING    â”‚
â”‚  to main  â”‚    â”‚  10 min  â”‚    â”‚  15 min  â”‚    â”‚  auto-deploy â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                 â”‚  PRODUCTION  â”‚
                                                 â”‚ manual gate  â”‚
                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Deployment Strategy Comparison

| Strategy | Downtime | Risk | Suitable for Voice? |
|----------|----------|------|---------------------|
| **Rolling Update** âœ… | None | Low | Yes â€” with connection draining |
| Blue-Green | None | Low | Yes â€” but 2x infrastructure cost |
| Canary | None | Very Low | Yes â€” best for gradual rollout |
| Recreate | 10-30s | High | âŒ No â€” drops active calls |

**Chosen: Rolling Update with Connection Draining**

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1        # Add 1 new pod
    maxUnavailable: 0  # Never remove a pod until new one is ready
```

### 4.3 Zero-Downtime Deploy Sequence

```
T+0s:   New pod created with updated image
T+5s:   New pod passes startup probe
T+10s:  New pod passes readiness probe â†’ added to Service endpoints
T+10s:  Old pod marked for termination
T+10s:  preStop hook: sleep 5 (LB stops routing new traffic)
T+15s:  SIGTERM sent to old pod
T+15s:  App sets isShuttingDown=true, returns 503 to new requests
T+15s:  App drains active calls (up to 15s for in-flight calls)
T+30s:  Active calls complete â†’ transcript/lead saved â†’ DB disconnected
T+30s:  Process exit 0
T+60s:  (Safety) K8s sends SIGKILL if process still alive
```

**Key insight:** The `terminationGracePeriodSeconds: 60` gives active calls
up to 45 seconds to complete (60s - 5s preStop - 10s buffer). This covers
most call scenarios without dropping audio.

### 4.4 Rollback

```bash
# Automatic rollback on failed deploy (in CI/CD)
kubectl rollout undo deployment/ai-calling-agent -n ai-calling

# Manual rollback to specific revision
kubectl rollout undo deployment/ai-calling-agent --to-revision=3 -n ai-calling

# Check rollout history
kubectl rollout history deployment/ai-calling-agent -n ai-calling
```

---

## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## 5. PRODUCTION MONITORING
## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 5.1 Health Check Endpoints

| Endpoint | Purpose | K8s Probe | Interval |
|----------|---------|-----------|----------|
| `GET /health` | Liveness â€” is process alive? | livenessProbe | 15s |
| `GET /health/ready` | Readiness â€” can accept traffic? | readinessProbe | 10s |
| `GET /api/v1/metrics` | Full metrics dashboard | Prometheus scrape | 30s |

### 5.2 Metrics Exported

```json
{
  "calls": {
    "started": 1500,
    "completed": 1420,
    "failed": 80,
    "active": 5,
    "peakConcurrent": 12,
    "avgDuration": "45.2s",
    "successRate": "94.7%"
  },
  "pipeline": {
    "p50": 1250,
    "p95": 2800,
    "p99": 4200,
    "avgStt": 450,
    "avgLlm": 380,
    "avgTts": 320
  },
  "errors": {
    "sttErrors": 3,
    "llmErrors": 1,
    "ttsErrors": 2,
    "wsDisconnects": 5,
    "bufferOverflows": 0
  },
  "system": {
    "memoryMB": 87,
    "uptimeSec": 86400
  }
}
```

### 5.3 Alert Rules

| Alert | Condition | Severity | Action |
|-------|-----------|----------|--------|
| **High Pipeline Latency** | P95 > 3000ms for 5 min | ğŸ”´ CRITICAL | Check OpenAI status, scale pods |
| **High Error Rate** | Call failure > 10% for 5 min | ğŸ”´ CRITICAL | Check logs, rollback if post-deploy |
| **Memory Pressure** | RSS > 400MB for 10 min | ğŸŸ  WARNING | Check for memory leaks, restart pod |
| **Active Calls Spike** | > 15 concurrent calls | ğŸŸ¡ INFO | HPA should handle, verify scaling |
| **STT Failures** | > 5% error rate | ğŸŸ  WARNING | Check Whisper API, audio format |
| **WebSocket Drops** | > 10 disconnects/hour | ğŸŸ  WARNING | Check network, LB settings |
| **DB Disconnect** | Health ready = false | ğŸ”´ CRITICAL | Check MongoDB Atlas, connection pool |
| **Cost Spike** | Daily cost > â‚¹1000 | ğŸŸ  WARNING | Check call volume, budget limits |
| **Pod OOMKill** | Container restart reason: OOM | ğŸ”´ CRITICAL | Increase memory limits, investigate leak |
| **Deploy Failed** | Rollout stuck > 5 min | ğŸ”´ CRITICAL | Auto-rollback triggered |

### 5.4 Logging Architecture

```
Application (structured JSON logs)
    â”‚
    â”œâ”€â”€ stdout â†’ Container runtime
    â”‚               â”‚
    â”‚               â–¼
    â”‚           Fluentd / Fluent Bit (DaemonSet)
    â”‚               â”‚
    â”‚               â–¼
    â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚  Log Aggregation              â”‚
    â”‚           â”‚  - AWS CloudWatch Logs        â”‚
    â”‚           â”‚  - Datadog Log Management     â”‚
    â”‚           â”‚  - ELK Stack (self-hosted)    â”‚
    â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€ Per-call tracing:
    â”‚   callSid attached to every log line
    â”‚   Filter: callSid=CA1234... shows full call lifecycle
    â”‚
    â””â”€â”€ Retention:
        - Hot: 7 days (searchable)
        - Warm: 30 days (compressed)
        - Archive: 90 days (S3 lifecycle)
```

---

## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## 6. SCALING STRATEGY
## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 6.1 Capacity Planning

Each pod can handle ~5-10 concurrent calls (CPU-bound during STT+LLM+TTS).

| Tier | Concurrent Calls | Pods | CPU Total | Memory Total | Monthly Cost (est.) |
|------|-------------------|------|-----------|--------------|---------------------|
| **Small** | 10 | 2 | 2 vCPU | 1 GB | ~$50 infra |
| **Medium** | 100 | 12-15 | 15 vCPU | 8 GB | ~$300 infra |
| **Large** | 1000 | 120-150 | 150 vCPU | 80 GB | ~$2500 infra |

> **Note:** Infrastructure cost is tiny compared to API costs.
> At 1000 concurrent calls, OpenAI API alone costs ~$500-1000/day.

### 6.2 Scaling per Tier

#### Tier 1: 10 Concurrent Calls (current)

```
Infrastructure:
  - 2 pods (m5.large or t3.medium)
  - MongoDB Atlas M10 (shared RAM)
  - Redis: not required
  - Single AZ acceptable (cost savings)

Bottleneck:    OpenAI API rate limits
Upgrade path:  Increase pod count â†’ HPA handles
Monthly infra: ~â‚¹4,000 ($50)
Monthly API:   ~â‚¹8,000-20,000 ($100-250)
```

#### Tier 2: 100 Concurrent Calls

```
Infrastructure:
  - 12-15 pods with HPA (min:4, max:20)
  - MongoDB Atlas M30 (dedicated, 8GB RAM)
  - Redis ElastiCache (rate limiting, session sticky)
  - Multi-AZ required
  - Nginx with upstream hash for WebSocket stickiness

Bottleneck:    OpenAI API throughput, DB connection pool
Upgrades:
  âœ… Redis for distributed rate limiting (replace in-memory Map)
  âœ… MongoDB connection pool increase (20 â†’ 50)
  âœ… OpenAI Tier 3+ API access (higher rate limits)
  âœ… CDN for TTS audio (Cloudflare cache)
  âœ… Horizontal pod autoscaling: CPU 60% target

Monthly infra: ~â‚¹24,000 ($300)
Monthly API:   ~â‚¹80,000-200,000 ($1000-2500)
```

#### Tier 3: 1000 Concurrent Calls

```
Infrastructure:
  - 120-150 pods with HPA (min:20, max:200)
  - MongoDB Atlas M50+ (dedicated cluster, 32GB RAM, auto-scaling storage)
  - Redis Cluster (6 nodes, for distributed locking + pub/sub)
  - Multi-region deployment
  - Call queue system (Redis + BullMQ)

Architecture Changes Required:
  âœ… Call queue: Don't start all 1000 calls simultaneously
     â†’ Queue system with max concurrency per instance
  âœ… Redis Pub/Sub: Cross-pod communication for call events
  âœ… Distributed state: Move CallSession to Redis (not in-memory)
  âœ… OpenAI Enterprise: Dedicated API capacity
  âœ… Twilio Enterprise: Volume pricing, dedicated interconnects
  âœ… Multiple Twilio numbers: Spread across 10-20 phone numbers
  âœ… Regional deployment: India + US for latency optimization
  âœ… Database sharding: Separate DBs for calls, leads, transcripts

Monthly infra: ~â‚¹200,000 ($2500)
Monthly API:   ~â‚¹800,000-2,000,000 ($10,000-25,000)
```

### 6.3 Auto-Scaling Configuration

```yaml
# HPA scales based on CPU (voice processing is CPU-intensive)
metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60    # Lower than typical 80% â€” latency-sensitive

behavior:
  scaleUp:
    stabilizationWindowSeconds: 60  # React within 1 minute
    policies:
      - type: Pods
        value: 2
        periodSeconds: 60  # Add 2 pods per minute max

  scaleDown:
    stabilizationWindowSeconds: 300  # Wait 5 min before removing pods
    policies:
      - type: Pods
        value: 1
        periodSeconds: 120  # Remove 1 pod every 2 min
```

**Why 60% CPU target (not 80%)?**
Voice is latency-sensitive. At 80% CPU, the STT+LLM+TTS pipeline competes
for CPU time with audio processing, causing 500ms+ additional latency.
60% leaves headroom for burst processing during concurrent pipelines.

**Why slow scale-down?**
Active calls hold WebSocket connections to specific pods. Removing a pod
drops those calls. The 5-minute stabilization window and 1-pod-per-2-min
policy minimizes disruption.

### 6.4 Queue Strategy (at 100+ calls)

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚       API: Start Campaign    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚      Redis Queue (BullMQ)    â”‚
                   â”‚  - Priority queue            â”‚
                   â”‚  - Max concurrency: 50       â”‚
                   â”‚  - Retry with backoff        â”‚
                   â”‚  - Dead letter queue         â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚              â”‚              â”‚
              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
              â”‚ Worker 1â”‚   â”‚ Worker 2â”‚   â”‚ Worker Nâ”‚
              â”‚ 10 callsâ”‚   â”‚ 10 callsâ”‚   â”‚ 10 callsâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
  - Backpressure control (don't overwhelm Twilio/OpenAI)
  - Retry failed calls automatically
  - Priority scheduling (hot leads first)
  - Campaign pause/resume
  - Rate limiting per campaign
```

---

## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## 7. COST OPTIMIZATION
## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 7.1 Cost Breakdown (per call)

| Component | Cost per Minute | Cost per 5-min Call |
|-----------|----------------|---------------------|
| Twilio Voice | â‚¹1.5 ($0.018) | â‚¹7.5 |
| Twilio + India mobile | â‚¹3.0 ($0.036) | â‚¹15.0 |
| OpenAI Whisper (STT) | â‚¹0.5 ($0.006) | â‚¹2.5 |
| OpenAI GPT-4o-mini | â‚¹0.4 ($0.005) | â‚¹2.0 |
| OpenAI TTS | â‚¹1.2 ($0.015) | â‚¹6.0 |
| S3/R2 Storage | â‚¹0.01 | â‚¹0.05 |
| Infrastructure | â‚¹0.1 | â‚¹0.5 |
| **TOTAL** | **â‚¹6.7/min** | **â‚¹33.5/call** |

### 7.2 Optimization Strategies

| Strategy | Savings | Effort |
|----------|---------|--------|
| TTS caching (common phrases) | 15-20% on TTS costs | âœ… Already implemented |
| GPT-4o-mini (not GPT-4) | 90% on LLM costs | âœ… Already using |
| Whisper API (not Whisper Large local) | N/A â€” API is simpler | Current approach |
| S3 â†’ R2 (no egress fees) | 100% on storage egress | âœ… Already using R2 |
| Reserved/spot instances (infra) | 30-60% on compute | Medium effort |
| Twilio committed use | 10-25% on telephony | Negotiation |
| OpenAI Tier 4+ pricing | Better rate limits | Volume-based |
| Silence detection (skip empty STT) | 10% on STT costs | âœ… Already implemented |
| Short audio filter (<125ms) | 5% on STT costs | âœ… Already implemented |
| CDN for TTS audio | Faster playback, lower S3 reads | Low effort |

---

## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## 8. DEPLOYMENT CHECKLIST
## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### Pre-Deploy
- [ ] All 40 unit tests passing
- [ ] Docker image builds successfully
- [ ] Docker image runs and passes health check locally
- [ ] All secrets configured in secrets manager
- [ ] MongoDB Atlas cluster provisioned and accessible
- [ ] S3/R2 bucket created with correct permissions
- [ ] Twilio phone numbers provisioned
- [ ] Twilio webhook URLs updated to production domain
- [ ] SSL certificate obtained and configured
- [ ] DNS records pointing to load balancer
- [ ] Nginx/Ingress configured with WebSocket support

### Deploy
- [ ] Push to main branch
- [ ] CI pipeline passes (test â†’ build â†’ staging)
- [ ] Staging health check passes
- [ ] Staging smoke test: make a test call
- [ ] Approve production deploy
- [ ] Production health check passes

### Post-Deploy
- [ ] `GET /health` returns `{"ok":true}`
- [ ] `GET /health/ready` returns database connected
- [ ] `GET /api/v1/metrics` returns valid metrics
- [ ] Make a real test call â€” verify greeting plays within 2s
- [ ] Speak to AI â€” verify response within 3s
- [ ] Be silent for 10s â€” verify "Are you still there?" prompt
- [ ] Re-verify MongoDB has Call, Transcript, Lead records
- [ ] Verify S3/R2 has TTS audio files
- [ ] Check logs for any errors
- [ ] Verify Twilio console shows successful call

### Ongoing
- [ ] Daily: Check P95 latency < 3000ms
- [ ] Daily: Check call success rate > 90%
- [ ] Daily: Check memory usage < 200MB
- [ ] Weekly: Review cost vs budget
- [ ] Weekly: Check MongoDB backup status
- [ ] Weekly: Review error logs for patterns
- [ ] Monthly: Update Node.js base image
- [ ] Monthly: Run `npm audit` for vulnerabilities
- [ ] Monthly: Rotate Twilio auth token

---

## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
## 9. FILES DELIVERED
## â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

| File | Purpose |
|------|---------|
| `Dockerfile` | Multi-stage production image (95MB, non-root, tini) |
| `.dockerignore` | Exclude dev files from build context |
| `docker-compose.yml` | Local development with MongoDB + Redis |
| `deploy/nginx.conf` | SSL, WebSocket, rate limiting, DDoS protection |
| `deploy/k8s/deployment.yaml` | K8s namespace, deployment, service, ingress, HPA, PDB |
| `.github/workflows/deploy.yml` | 4-stage CI/CD with auto-rollback |
| `scripts/load-test.js` | WebSocket load tester (simulates real Twilio streams) |
| `src/server.js` | Enhanced with liveness/readiness probes, connection draining, HSTS |

---

*Report generated: 2026-02-14 | Architecture version: 2.0.0*
